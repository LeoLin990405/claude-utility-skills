# Anthropic 工程博客详细笔记

> 来源: https://www.anthropic.com/engineering
> 整理日期: 2026-01-30

---

## 1. AI 抗性技术评估设计 (2026-01-21)

### 核心问题
随着 AI 能力提升，技术候选人评估变得困难。今天有效的测试，明天可能被模型轻松解决。

### 演进历程

**原始测试 (2023年11月)**
- 构建模拟 TPU 的 Python 模拟器
- 特性：手动管理暂存内存、VLIW、SIMD、多核
- 任务：并行树遍历优化
- 约1000名候选人完成，帮助招聘大部分性能工程团队

**第一次失败：Claude Opus 4 (2025年5月)**
- Claude 3.7 Sonnet 已达到50%候选人不如直接委托给 Claude Code
- Claude Opus 4 在4小时内超过几乎所有人类
- 解决方案：版本2，使用 Claude 识别其困难点作为新起点

**第二次失败：Claude Opus 4.5**
- 2小时内得分匹配最佳人类表现
- 甚至找到作者未想到的优化

**最终解决方案：走向怪异**
- 灵感来自 Zachtronics 游戏
- 使用微小、高度受限指令集的谜题
- 故意不提供可视化或调试工具
- 构建调试工具是测试内容的一部分

### 关键洞察
1. AI 能力提升速度超过预期
2. 现实性与 AI 抗性的权衡：越真实的问题，AI 训练数据越多
3. 分布外是关键：新颖、不寻常的问题更能抵抗 AI

---

## 2. AI Agent 评估揭秘 (2026-01-09)

### 核心观点
良好的评估帮助团队更自信地交付 AI Agent，使问题在影响用户之前被发现。

### 评估术语体系
- **Task**: 具有明确输入和成功标准的单个测试
- **Trial**: 对任务的一次尝试
- **Grader**: 评估 Agent 性能的逻辑
- **Transcript**: 完整的试验记录
- **Outcome**: 试验结束时的最终状态

### 三种评分器类型

| 类型 | 方法 | 优势 | 劣势 |
|------|------|------|------|
| 代码评分器 | 字符串匹配、测试、静态分析 | 快速、便宜、客观 | 对有效变体脆弱 |
| 模型评分器 | 规则评分、自然语言断言 | 灵活、可扩展 | 非确定性、更贵 |
| 人工评分器 | 专家审查、众包判断 | 黄金标准质量 | 昂贵、缓慢 |

### 处理非确定性
- **pass@k**: k次尝试中至少一次成功的概率
- **pass^k**: 所有k次试验都成功的概率

### 从零到一路线图
1. 从20-50个简单任务开始
2. 转换用户报告的失败为测试用例
3. 构建稳健的评估框架
4. 定期阅读评估记录和评分
5. 监控能力评估饱和

---

## 3. 长时间运行 Agent 的有效框架 (2025-11-26)

### 核心挑战
跨多个上下文窗口保持一致进展。每个新会话开始时都没有之前的记忆。

### 解决方案：双重 Agent 系统

**初始化 Agent**
- 创建 `init.sh` 脚本
- 建立 `claude-progress.txt` 日志文件
- 生成初始 git 提交记录

**编码 Agent**
- 增量推进任务
- 为下一会话留下清晰的工作痕迹

### 主要失败模式及解决方案

| 失败模式 | 解决方案 |
|----------|----------|
| 过早宣布完成 | 创建200+功能的详细特性列表(JSON) |
| 留下混乱状态 | 使用 git 提交和 progress 文件 |
| 测试不充分 | 使用 Puppeteer MCP 进行浏览器测试 |
| 重复设置工作 | 初始化 Agent 编写 init.sh 脚本 |

---

## 4. 高级工具使用功能 (2025-11-24)

### 三个新 Beta 功能

**1. Tool Search Tool**
- 按需发现工具，节省 85% token 使用量
- 准确率提升：Opus 4.5 从 79.5% → 88.1%

```json
{
  "type": "tool_search_tool_regex_20251119",
  "name": "tool_search_tool_regex"
}
```

**2. Programmatic Tool Calling**
- Claude 通过 Python 代码编排工具
- Token 使用量减少 37%

```python
tools=[
    {"type": "code_execution_20250825", "name": "code_execution"},
    {
        "name": "query_database",
        "allowed_callers": ["code_execution_20250825"]
    }
]
```

**3. Tool Use Examples**
- 提供具体使用示例
- 内部测试准确率从 72% → 90%

---

## 5. MCP 代码执行 (2025-11-04)

### 核心问题
- 工具定义过载上下文窗口
- 中间结果消耗额外 token

### 解决方案
将 MCP 服务器呈现为代码 API，而非直接工具调用。

**效率提升**: Token 使用从 150,000 降至 2,000 - **节省 98.7%**

### 核心优势
1. 渐进式披露：按需加载工具定义
2. 上下文高效：在执行环境中过滤数据
3. 更强大的控制流：循环、条件、错误处理
4. 隐私保护：中间结果不经过模型
5. 状态持久化和技能

---

## 6. Claude Code 沙箱安全 (2025-10-20)

### 双重隔离机制

**文件系统隔离**
- 限制 Claude 只能访问特定目录
- 防止修改敏感系统文件

**网络隔离**
- 确保只能连接已批准的服务器
- 防止数据泄露或下载恶意软件

### 效果
内部测试显示，沙箱技术安全地减少了 **84%** 的权限提示。

### 两个新功能
1. **沙箱化 Bash 工具**: 基于 Linux bubblewrap 和 macOS seatbelt
2. **Web 版 Claude Code**: 云端隔离沙箱执行

---

## 7. 有效上下文工程 (2025-09-29)

### 核心概念
**Context Engineering**: 在 LLM 推理过程中策划和维护最优 token 集合的策略。

### 核心原则
> "find the smallest possible set of high-signal tokens that maximize the likelihood of some desired outcome"

### 上下文腐化
随着上下文窗口中 token 数量增加，模型准确召回信息的能力下降。

### 有效上下文的构成
1. **系统提示**: 清晰、简单、直接的语言
2. **工具**: 功能最小重叠，自包含、错误健壮
3. **示例**: 多样化、规范化的示例集

### 长时程任务策略
1. **压缩**: 总结内容并用摘要重新启动
2. **结构化笔记**: 定期写入持久内存
3. **子智能体架构**: 主智能体协调，子智能体处理聚焦任务

---

## 8. 三个基础设施 Bug 事后分析 (2025-09-17)

### 三个主要 Bug

**1. 上下文窗口路由错误**
- 短上下文请求被错误路由到 1M token 服务器
- 最严重时影响 16% 的请求

**2. 输出损坏**
- Token 生成过程中出现错误
- 英文提示中出现泰语或中文字符

**3. XLA:TPU 编译错误**
- 触发了编译器中的潜在 bug
- 混合精度算术问题

### 检测困难的原因
1. 评估未能捕获用户报告的退化
2. 隐私保护限制调试
3. 症状多样性
4. 过度依赖噪声评估

---

## 9. 为 Agent 编写有效工具 (2025-09-11)

### 核心洞察
工具质量直接决定代理效能。传统软件开发范式不适用于代理工具开发。

### 迭代开发流程
1. 构建原型
2. 运行评估
3. 协作优化

### 核心设计原则

**1. 选择正确的工具**
- 更多工具 ≠ 更好结果
- 实现 `search_contacts` 而非 `list_contacts`

**2. 命名空间**
- 按服务：`asana_search`、`jira_search`
- 按资源：`asana_projects_search`

**3. 返回有意义的上下文**
- 避免低级标识符
- 使用自然语言标识符

**4. Token 效率优化**
- 分页、范围选择、过滤、截断

**5. 提示工程工具描述**
- 像向新员工解释一样描述工具

---

## 10. Desktop Extensions (2025-06-26)

### 核心问题
MCP 服务器安装过程复杂：需要开发者工具、手动编辑配置、处理依赖冲突。

### 解决方案
Desktop Extensions (`.mcpb` 文件) 将整个 MCP 服务器及其依赖打包成单一可安装包。

**安装流程**:
- 之前：安装 Node.js → npm 安装 → 手动编辑配置 → 重启
- 现在：下载 `.mcpb` → 双击 → 点击"安装"

### 技术架构
```
extension.mcpb
├── manifest.json
├── server/
├── dependencies/
└── icon.png
```

### 四步构建流程
1. `npx @anthropic-ai/mcpb init`
2. 声明用户配置
3. `npx @anthropic-ai/mcpb pack`
4. 本地测试

---

## 11. 多智能体研究系统 (2025-06-13)

### 性能数据
- 多智能体系统比单智能体 Opus 4 **性能提升 90.2%**
- Token 消耗：多智能体系统比聊天多用约 15×

### 系统架构：编排者-工作者模式
```
用户查询 → 主智能体 → 创建并行子智能体 → 综合结果 → 返回
```

### 8大提示工程原则
1. 像智能体一样思考
2. 教会编排者如何委派
3. 根据查询复杂度扩展工作量
4. 工具设计和选择至关重要
5. 让智能体自我改进
6. 先宽后窄
7. 引导思考过程
8. 并行工具调用

### 三层评估策略
1. 小样本快速迭代
2. LLM 作为评判者
3. 人工评估

---

## 12. Claude Code 最佳实践 (2025-04-18)

### 核心约束
Claude 的上下文窗口填充快，性能随填充而下降。

### 最高杠杆建议
**给 Claude 验证自己工作的方式**：测试、截图、预期输出。

### 推荐工作流
1. **探索**: Plan Mode 读取文件
2. **计划**: 创建详细实现计划
3. **实现**: Normal Mode 编码
4. **提交**: 创建 PR

### CLAUDE.md 最佳实践
- 保持简短和人类可读
- 只包含 Claude 无法从代码推断的信息
- 定期修剪

### 会话管理
- `/clear`: 重置上下文
- `/rewind`: 恢复检查点
- `--continue`: 恢复最近会话

---

## 13. SWE-bench Verified 突破 (2025-01-06)

### 核心成果
Claude 3.5 Sonnet 在 SWE-bench Verified 达到 **49%**，超越此前 45% 的业界最佳。

### Agent 架构
- 一个提示词
- Bash 工具
- Edit 工具 (str_replace_editor)
- 持续采样直到完成或超出 200k 上下文

### 工具设计关键
- 要求绝对路径
- 字符串替换策略：`old_str` 必须精确匹配一次

### 技术洞察
> "much more attention should go into designing tool interfaces for models, in the same way that a large amount of attention goes into designing tool interfaces for humans"

---

## 14. 构建有效 Agent (2024-12-19)

### 核心观点
> 最成功的 LLM 代理实现使用简单、可组合的模式，而非复杂框架。

### Agentic Systems 两种类型
1. **Workflows**: 预定义代码路径编排
2. **Agents**: LLM 动态指导自己的流程

### 六种工作流模式

| 模式 | 描述 | 适用场景 |
|------|------|----------|
| Prompt Chaining | 序列步骤 | 可清晰分解的任务 |
| Routing | 输入分类导向 | 有明确类别的任务 |
| Parallelization | 并行运行 | 独立子任务 |
| Orchestrator-Workers | 动态分解委派 | 复杂代码修改 |
| Evaluator-Optimizer | 循环评估反馈 | 有明确评估标准 |
| Agents | 自主规划操作 | 开放式问题 |

### 三大核心原则
1. **Simplicity**: 保持设计简单
2. **Transparency**: 明确显示规划步骤
3. **Documentation and Testing**: 精心设计 ACI

### ACI 设计建议
- 像投入 HCI 一样投入 ACI 设计
- 工具定义应包括示例用法、边缘情况
- **Poka-yoke（防错）**: 修改参数使错误更难发生
